2025-01-25 13:10:04,176 saving log, checkpoint and back up code in folder: log/shapenetpart_cls-dense-trans-n7-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250125-131004_321eea3e-a93b-47e8-84cb-499db5287f76
2025-01-25 13:10:04,176 ==========       args      =============
2025-01-25 13:10:04,176 phase:train
2025-01-25 13:10:04,176 exp_name:shapenetpart_cls-dense-trans-n7-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1
2025-01-25 13:10:04,176 job_name:shapenetpart_cls-dense-trans-n7-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250125-131004_321eea3e-a93b-47e8-84cb-499db5287f76
2025-01-25 13:10:04,176 use_cpu:False
2025-01-25 13:10:04,176 root_dir:log
2025-01-25 13:10:04,176 data_dir:/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/dataset
2025-01-25 13:10:04,176 dataset:ShapeNetPart
2025-01-25 13:10:04,176 num_points:2048
2025-01-25 13:10:04,176  :True
2025-01-25 13:10:04,176 in_channels:3
2025-01-25 13:10:04,176 batch_size:32
2025-01-25 13:10:04,177 epochs:400
2025-01-25 13:10:04,177 use_sgd:False
2025-01-25 13:10:04,177 weight_decay:0.0001
2025-01-25 13:10:04,177 lr:0.001
2025-01-25 13:10:04,177 seed:1
2025-01-25 13:10:04,177 multi_gpus:True
2025-01-25 13:10:04,177 test_batch_size:50
2025-01-25 13:10:04,177 pretrained_model:
2025-01-25 13:10:04,177 k:15
2025-01-25 13:10:04,177 block:dense
2025-01-25 13:10:04,177 conv:trans
2025-01-25 13:10:04,177 act:relu
2025-01-25 13:10:04,177 norm:batch
2025-01-25 13:10:04,177 bias:True
2025-01-25 13:10:04,177 n_blocks:7
2025-01-25 13:10:04,177 n_filters:256
2025-01-25 13:10:04,177 emb_dims:1024
2025-01-25 13:10:04,177 dropout:0.5
2025-01-25 13:10:04,177 dynamic:True
2025-01-25 13:10:04,177 fine_tune:False
2025-01-25 13:10:04,177 fine_tune_num_classes:16
2025-01-25 13:10:04,177 use_dilation:True
2025-01-25 13:10:04,177 epsilon:0.2
2025-01-25 13:10:04,177 use_stochastic:True
2025-01-25 13:10:04,177 device:cuda
2025-01-25 13:10:04,177 exp_dir:log/shapenetpart_cls-dense-trans-n7-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250125-131004_321eea3e-a93b-47e8-84cb-499db5287f76
2025-01-25 13:10:04,177 ckpt_dir:log/shapenetpart_cls-dense-trans-n7-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250125-131004_321eea3e-a93b-47e8-84cb-499db5287f76/checkpoint
2025-01-25 13:10:04,177 code_dir:log/shapenetpart_cls-dense-trans-n7-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250125-131004_321eea3e-a93b-47e8-84cb-499db5287f76/code
2025-01-25 13:10:04,177 writer:<torch.utils.tensorboard.writer.SummaryWriter object at 0x14d1064d4e80>
2025-01-25 13:10:04,177 epoch:-1
2025-01-25 13:10:04,177 step:-1
2025-01-25 13:10:04,177 loglevel:info
2025-01-25 13:10:04,177 ==========     args END    =============
2025-01-25 13:10:04,178 

2025-01-25 13:10:04,178 ===> Phase is train.
2025-01-25 13:10:04,182 ===> Creating data-loader ...
2025-01-25 13:10:07,906 ===> Loading ShapeNetPart from /user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/dataset. number of classes equal to 16
2025-01-25 13:10:07,906 ===> Loading the network ...
2025-01-25 13:10:08,257 ===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
├─DynamicEdgeConvLayer: 1-1                        --
|    └─Sequential: 2-1                             --
|    |    └─Linear: 3-1                            1,792
|    |    └─ReLU: 3-2                              --
|    |    └─Linear: 3-3                            65,792
|    |    └─ReLU: 3-4                              --
|    └─DynamicEdgeConv: 2-2                        --
|    |    └─MaxAggregation: 3-5                    --
|    |    └─Sequential: 3-6                        (recursive)
|    └─BatchNorm1d: 2-3                            512
├─Sequential: 1-2                                  --
|    └─DenseDynBlock2d: 2-4                        --
|    |    └─DynConv2d: 3-7                         788,736
|    └─DenseDynBlock2d: 2-5                        --
|    |    └─DynConv2d: 3-8                         1,050,880
|    └─DenseDynBlock2d: 2-6                        --
|    |    └─DynConv2d: 3-9                         1,313,024
|    └─DenseDynBlock2d: 2-7                        --
|    |    └─DynConv2d: 3-10                        1,575,168
|    └─DenseDynBlock2d: 2-8                        --
|    |    └─DynConv2d: 3-11                        1,837,312
|    └─DenseDynBlock2d: 2-9                        --
|    |    └─DynConv2d: 3-12                        2,099,456
├─BasicConv: 1-3                                   --
|    └─Conv2d: 2-10                                7,340,032
|    └─LeakyReLU: 2-11                             --
|    └─BatchNorm2d: 2-12                           2,048
├─Sequential: 1-4                                  --
|    └─BasicConv: 2-13                             --
|    |    └─Conv2d: 3-13                           1,049,088
|    |    └─LeakyReLU: 3-14                        --
|    |    └─BatchNorm2d: 3-15                      1,024
|    |    └─Dropout2d: 3-16                        --
|    └─BasicConv: 2-14                             --
|    |    └─Conv2d: 3-17                           131,328
|    |    └─LeakyReLU: 3-18                        --
|    |    └─BatchNorm2d: 3-19                      512
|    |    └─Dropout2d: 3-20                        --
|    └─BasicConv: 2-15                             --
|    |    └─Conv2d: 3-21                           4,112
===========================================================================
Total params: 17,260,816
Trainable params: 17,260,816
Non-trainable params: 0
===========================================================================
2025-01-25 13:10:08,447 DataParallel(
  (module): DeepGCN(
    (dynamic_head): DynamicEdgeConvLayer(
      (mlp): Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      )
      (conv): DynamicEdgeConv(nn=Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      ), k=15)
      (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (backbone): Sequential(
      (0): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(256, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
        )
      )
      (1): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(512, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
        )
      )
      (2): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(768, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
        )
      )
      (3): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1024, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
        )
      )
      (4): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1280, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
        )
      )
      (5): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1536, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
        )
      )
    )
    (fusion_block): BasicConv(
      (0): Conv2d(7168, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): LeakyReLU(negative_slope=0.2)
      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (prediction): Sequential(
      (0): BasicConv(
        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (1): BasicConv(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (2): BasicConv(
        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
2025-01-25 13:10:08,448 ===> loading pre-trained ...
2025-01-25 13:10:08,448 ===> No pre-trained model
2025-01-25 13:10:08,448 ===> Init the optimizer ...
2025-01-25 13:10:08,448 ===> Use AdamW
2025-01-25 13:10:08,449 ===> Init Metric ...
2025-01-25 13:10:08,453 ===> start training ...
