2025-01-25 22:24:37,732 saving log, checkpoint and back up code in folder: log
2025-01-25 22:24:37,732 ==========       args      =============
2025-01-25 22:24:37,732 phase:test
2025-01-25 22:24:37,732 exp_name:
2025-01-25 22:24:37,732 job_name:modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0
2025-01-25 22:24:37,733 use_cpu:False
2025-01-25 22:24:37,733 root_dir:log
2025-01-25 22:24:37,733 data_dir:Dataset
2025-01-25 22:24:37,733 dataset:ModelNet40
2025-01-25 22:24:37,733 num_points:2048
2025-01-25 22:24:37,733  :True
2025-01-25 22:24:37,733 in_channels:3
2025-01-25 22:24:37,733 batch_size:32
2025-01-25 22:24:37,733 epochs:400
2025-01-25 22:24:37,733 use_sgd:False
2025-01-25 22:24:37,733 weight_decay:0.0001
2025-01-25 22:24:37,733 lr:0.001
2025-01-25 22:24:37,733 seed:1
2025-01-25 22:24:37,733 multi_gpus:True
2025-01-25 22:24:37,733 test_batch_size:50
2025-01-25 22:24:37,733 pretrained_model:/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1-best_model.pth
2025-01-25 22:24:37,733 k:15
2025-01-25 22:24:37,733 block:dense
2025-01-25 22:24:37,733 conv:trans
2025-01-25 22:24:37,733 act:relu
2025-01-25 22:24:37,733 norm:batch
2025-01-25 22:24:37,733 bias:True
2025-01-25 22:24:37,733 n_blocks:7
2025-01-25 22:24:37,733 n_filters:256
2025-01-25 22:24:37,733 emb_dims:1024
2025-01-25 22:24:37,733 dropout:0.5
2025-01-25 22:24:37,733 dynamic:True
2025-01-25 22:24:37,733 fine_tune:False
2025-01-25 22:24:37,733 fine_tune_num_classes:16
2025-01-25 22:24:37,733 use_dilation:True
2025-01-25 22:24:37,733 epsilon:0.2
2025-01-25 22:24:37,733 use_stochastic:True
2025-01-25 22:24:37,734 device:cuda
2025-01-25 22:24:37,734 exp_dir:log
2025-01-25 22:24:37,734 res_dir:/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/result/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0
2025-01-25 22:24:37,734 loglevel:info
2025-01-25 22:24:37,734 ==========     args END    =============
2025-01-25 22:24:37,734 

2025-01-25 22:24:37,734 ===> Phase is test.
2025-01-25 22:24:37,739 ===> Creating data-loader ...
2025-01-25 22:24:41,332 ===> Loading ModelNet40 from Dataset. number of classes equal to 40
2025-01-25 22:24:41,333 ===> Loading the network ...
2025-01-25 22:24:41,693 ===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
├─DynamicEdgeConvLayer: 1-1                        --
|    └─Sequential: 2-1                             --
|    |    └─Linear: 3-1                            1,792
|    |    └─ReLU: 3-2                              --
|    |    └─Linear: 3-3                            65,792
|    |    └─ReLU: 3-4                              --
|    └─DynamicEdgeConv: 2-2                        --
|    |    └─MaxAggregation: 3-5                    --
|    |    └─Sequential: 3-6                        (recursive)
|    └─BatchNorm1d: 2-3                            512
├─Sequential: 1-2                                  --
|    └─DenseDynBlock2d: 2-4                        --
|    |    └─DynConv2d: 3-7                         526,080
|    └─DenseDynBlock2d: 2-5                        --
|    |    └─DynConv2d: 3-8                         788,224
|    └─DenseDynBlock2d: 2-6                        --
|    |    └─DynConv2d: 3-9                         1,050,368
|    └─DenseDynBlock2d: 2-7                        --
|    |    └─DynConv2d: 3-10                        1,312,512
|    └─DenseDynBlock2d: 2-8                        --
|    |    └─DynConv2d: 3-11                        1,574,656
|    └─DenseDynBlock2d: 2-9                        --
|    |    └─DynConv2d: 3-12                        1,836,800
├─BasicConv: 1-3                                   --
|    └─Conv2d: 2-10                                7,340,032
|    └─LeakyReLU: 2-11                             --
|    └─BatchNorm2d: 2-12                           2,048
├─Sequential: 1-4                                  --
|    └─BasicConv: 2-13                             --
|    |    └─Conv2d: 3-13                           1,049,088
|    |    └─LeakyReLU: 3-14                        --
|    |    └─BatchNorm2d: 3-15                      1,024
|    |    └─Dropout2d: 3-16                        --
|    └─BasicConv: 2-14                             --
|    |    └─Conv2d: 3-17                           131,328
|    |    └─LeakyReLU: 3-18                        --
|    |    └─BatchNorm2d: 3-19                      512
|    |    └─Dropout2d: 3-20                        --
|    └─BasicConv: 2-15                             --
|    |    └─Conv2d: 3-21                           10,280
===========================================================================
Total params: 15,691,048
Trainable params: 15,691,048
Non-trainable params: 0
===========================================================================
2025-01-25 22:24:41,880 DataParallel(
  (module): DeepGCN(
    (dynamic_head): DynamicEdgeConvLayer(
      (mlp): Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      )
      (conv): DynamicEdgeConv(nn=Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      ), k=15)
      (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (backbone): Sequential(
      (0): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(256, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (1): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(512, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (2): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(768, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (3): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1024, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (4): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1280, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (5): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1536, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
    )
    (fusion_block): BasicConv(
      (0): Conv2d(7168, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): LeakyReLU(negative_slope=0.2)
      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (prediction): Sequential(
      (0): BasicConv(
        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (1): BasicConv(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (2): BasicConv(
        (0): Conv2d(256, 40, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
2025-01-25 22:24:41,884 ===> loading pre-trained ...
2025-01-25 22:24:41,885 ===> Loading checkpoint '/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1-best_model.pth'
2025-01-25 22:34:54,315 saving log, checkpoint and back up code in folder: log
2025-01-25 22:34:54,316 ==========       args      =============
2025-01-25 22:34:54,316 phase:test
2025-01-25 22:34:54,316 exp_name:
2025-01-25 22:34:54,316 job_name:modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0
2025-01-25 22:34:54,316 use_cpu:False
2025-01-25 22:34:54,316 root_dir:log
2025-01-25 22:34:54,316 data_dir:Dataset
2025-01-25 22:34:54,316 dataset:ModelNet40
2025-01-25 22:34:54,316 num_points:2048
2025-01-25 22:34:54,316  :True
2025-01-25 22:34:54,316 in_channels:3
2025-01-25 22:34:54,316 batch_size:32
2025-01-25 22:34:54,316 epochs:400
2025-01-25 22:34:54,316 use_sgd:False
2025-01-25 22:34:54,316 weight_decay:0.0001
2025-01-25 22:34:54,316 lr:0.001
2025-01-25 22:34:54,316 seed:1
2025-01-25 22:34:54,316 multi_gpus:True
2025-01-25 22:34:54,316 test_batch_size:50
2025-01-25 22:34:54,316 pretrained_model:/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1-best_model.pth
2025-01-25 22:34:54,316 k:15
2025-01-25 22:34:54,316 block:dense
2025-01-25 22:34:54,316 conv:trans
2025-01-25 22:34:54,316 act:relu
2025-01-25 22:34:54,316 norm:batch
2025-01-25 22:34:54,316 bias:True
2025-01-25 22:34:54,316 n_blocks:6
2025-01-25 22:34:54,316 n_filters:256
2025-01-25 22:34:54,316 emb_dims:1024
2025-01-25 22:34:54,316 dropout:0.5
2025-01-25 22:34:54,316 dynamic:True
2025-01-25 22:34:54,316 fine_tune:False
2025-01-25 22:34:54,317 fine_tune_num_classes:16
2025-01-25 22:34:54,317 use_dilation:True
2025-01-25 22:34:54,317 epsilon:0.2
2025-01-25 22:34:54,317 use_stochastic:True
2025-01-25 22:34:54,317 device:cuda
2025-01-25 22:34:54,317 exp_dir:log
2025-01-25 22:34:54,317 res_dir:/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/result/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0
2025-01-25 22:34:54,317 loglevel:info
2025-01-25 22:34:54,317 ==========     args END    =============
2025-01-25 22:34:54,317 

2025-01-25 22:34:54,317 ===> Phase is test.
2025-01-25 22:34:54,321 ===> Creating data-loader ...
2025-01-25 22:34:57,970 ===> Loading ModelNet40 from Dataset. number of classes equal to 40
2025-01-25 22:34:57,970 ===> Loading the network ...
2025-01-25 22:34:58,255 ===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
├─DynamicEdgeConvLayer: 1-1                        --
|    └─Sequential: 2-1                             --
|    |    └─Linear: 3-1                            1,792
|    |    └─ReLU: 3-2                              --
|    |    └─Linear: 3-3                            65,792
|    |    └─ReLU: 3-4                              --
|    └─DynamicEdgeConv: 2-2                        --
|    |    └─MaxAggregation: 3-5                    --
|    |    └─Sequential: 3-6                        (recursive)
|    └─BatchNorm1d: 2-3                            512
├─Sequential: 1-2                                  --
|    └─DenseDynBlock2d: 2-4                        --
|    |    └─DynConv2d: 3-7                         526,080
|    └─DenseDynBlock2d: 2-5                        --
|    |    └─DynConv2d: 3-8                         788,224
|    └─DenseDynBlock2d: 2-6                        --
|    |    └─DynConv2d: 3-9                         1,050,368
|    └─DenseDynBlock2d: 2-7                        --
|    |    └─DynConv2d: 3-10                        1,312,512
|    └─DenseDynBlock2d: 2-8                        --
|    |    └─DynConv2d: 3-11                        1,574,656
├─BasicConv: 1-3                                   --
|    └─Conv2d: 2-9                                 5,505,024
|    └─LeakyReLU: 2-10                             --
|    └─BatchNorm2d: 2-11                           2,048
├─Sequential: 1-4                                  --
|    └─BasicConv: 2-12                             --
|    |    └─Conv2d: 3-12                           1,049,088
|    |    └─LeakyReLU: 3-13                        --
|    |    └─BatchNorm2d: 3-14                      1,024
|    |    └─Dropout2d: 3-15                        --
|    └─BasicConv: 2-13                             --
|    |    └─Conv2d: 3-16                           131,328
|    |    └─LeakyReLU: 3-17                        --
|    |    └─BatchNorm2d: 3-18                      512
|    |    └─Dropout2d: 3-19                        --
|    └─BasicConv: 2-14                             --
|    |    └─Conv2d: 3-20                           10,280
===========================================================================
Total params: 12,019,240
Trainable params: 12,019,240
Non-trainable params: 0
===========================================================================
2025-01-25 22:34:58,432 DataParallel(
  (module): DeepGCN(
    (dynamic_head): DynamicEdgeConvLayer(
      (mlp): Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      )
      (conv): DynamicEdgeConv(nn=Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      ), k=15)
      (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (backbone): Sequential(
      (0): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(256, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (1): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(512, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (2): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(768, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (3): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1024, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (4): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1280, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
    )
    (fusion_block): BasicConv(
      (0): Conv2d(5376, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): LeakyReLU(negative_slope=0.2)
      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (prediction): Sequential(
      (0): BasicConv(
        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (1): BasicConv(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (2): BasicConv(
        (0): Conv2d(256, 40, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
2025-01-25 22:34:58,436 ===> loading pre-trained ...
2025-01-25 22:34:58,437 ===> Loading checkpoint '/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1-best_model.pth'
2025-01-25 22:34:58,615 The pretrained_model is at checkpoint 108.
2025-01-25 22:35:15,974 Test Overall Acc 0.9234, Its test avg acc 0.8825.
2025-01-25 22:36:27,913 saving log, checkpoint and back up code in folder: log
2025-01-25 22:36:27,913 ==========       args      =============
2025-01-25 22:36:27,913 phase:test
2025-01-25 22:36:27,913 exp_name:
2025-01-25 22:36:27,913 job_name:modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0
2025-01-25 22:36:27,913 use_cpu:False
2025-01-25 22:36:27,913 root_dir:log
2025-01-25 22:36:27,913 data_dir:Dataset
2025-01-25 22:36:27,913 dataset:ModelNet40
2025-01-25 22:36:27,913 num_points:2048
2025-01-25 22:36:27,913  :True
2025-01-25 22:36:27,913 in_channels:3
2025-01-25 22:36:27,913 batch_size:32
2025-01-25 22:36:27,913 epochs:400
2025-01-25 22:36:27,913 use_sgd:False
2025-01-25 22:36:27,913 weight_decay:0.0001
2025-01-25 22:36:27,913 lr:0.001
2025-01-25 22:36:27,913 seed:1
2025-01-25 22:36:27,913 multi_gpus:True
2025-01-25 22:36:27,913 test_batch_size:50
2025-01-25 22:36:27,913 pretrained_model:/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1-best_model.pth
2025-01-25 22:36:27,913 k:15
2025-01-25 22:36:27,913 block:dense
2025-01-25 22:36:27,913 conv:trans
2025-01-25 22:36:27,913 act:relu
2025-01-25 22:36:27,914 norm:batch
2025-01-25 22:36:27,914 bias:True
2025-01-25 22:36:27,914 n_blocks:6
2025-01-25 22:36:27,914 n_filters:256
2025-01-25 22:36:27,914 emb_dims:1024
2025-01-25 22:36:27,914 dropout:0.5
2025-01-25 22:36:27,914 dynamic:True
2025-01-25 22:36:27,914 fine_tune:False
2025-01-25 22:36:27,914 fine_tune_num_classes:16
2025-01-25 22:36:27,914 use_dilation:True
2025-01-25 22:36:27,914 epsilon:0.2
2025-01-25 22:36:27,914 use_stochastic:True
2025-01-25 22:36:27,914 device:cuda
2025-01-25 22:36:27,914 exp_dir:log
2025-01-25 22:36:27,914 res_dir:/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/result/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0
2025-01-25 22:36:27,914 loglevel:info
2025-01-25 22:36:27,914 ==========     args END    =============
2025-01-25 22:36:27,914 

2025-01-25 22:36:27,914 ===> Phase is test.
2025-01-25 22:36:27,919 ===> Creating data-loader ...
2025-01-25 22:36:31,536 ===> Loading ModelNet40 from Dataset. number of classes equal to 40
2025-01-25 22:36:31,536 ===> Loading the network ...
2025-01-25 22:36:31,801 ===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
├─DynamicEdgeConvLayer: 1-1                        --
|    └─Sequential: 2-1                             --
|    |    └─Linear: 3-1                            1,792
|    |    └─ReLU: 3-2                              --
|    |    └─Linear: 3-3                            65,792
|    |    └─ReLU: 3-4                              --
|    └─DynamicEdgeConv: 2-2                        --
|    |    └─MaxAggregation: 3-5                    --
|    |    └─Sequential: 3-6                        (recursive)
|    └─BatchNorm1d: 2-3                            512
├─Sequential: 1-2                                  --
|    └─DenseDynBlock2d: 2-4                        --
|    |    └─DynConv2d: 3-7                         526,080
|    └─DenseDynBlock2d: 2-5                        --
|    |    └─DynConv2d: 3-8                         788,224
|    └─DenseDynBlock2d: 2-6                        --
|    |    └─DynConv2d: 3-9                         1,050,368
|    └─DenseDynBlock2d: 2-7                        --
|    |    └─DynConv2d: 3-10                        1,312,512
|    └─DenseDynBlock2d: 2-8                        --
|    |    └─DynConv2d: 3-11                        1,574,656
├─BasicConv: 1-3                                   --
|    └─Conv2d: 2-9                                 5,505,024
|    └─LeakyReLU: 2-10                             --
|    └─BatchNorm2d: 2-11                           2,048
├─Sequential: 1-4                                  --
|    └─BasicConv: 2-12                             --
|    |    └─Conv2d: 3-12                           1,049,088
|    |    └─LeakyReLU: 3-13                        --
|    |    └─BatchNorm2d: 3-14                      1,024
|    |    └─Dropout2d: 3-15                        --
|    └─BasicConv: 2-13                             --
|    |    └─Conv2d: 3-16                           131,328
|    |    └─LeakyReLU: 3-17                        --
|    |    └─BatchNorm2d: 3-18                      512
|    |    └─Dropout2d: 3-19                        --
|    └─BasicConv: 2-14                             --
|    |    └─Conv2d: 3-20                           10,280
===========================================================================
Total params: 12,019,240
Trainable params: 12,019,240
Non-trainable params: 0
===========================================================================
2025-01-25 22:36:31,983 DataParallel(
  (module): DeepGCN(
    (dynamic_head): DynamicEdgeConvLayer(
      (mlp): Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      )
      (conv): DynamicEdgeConv(nn=Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      ), k=15)
      (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (backbone): Sequential(
      (0): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(256, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (1): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(512, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (2): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(768, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (3): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1024, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (4): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1280, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
    )
    (fusion_block): BasicConv(
      (0): Conv2d(5376, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): LeakyReLU(negative_slope=0.2)
      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (prediction): Sequential(
      (0): BasicConv(
        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (1): BasicConv(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (2): BasicConv(
        (0): Conv2d(256, 40, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
2025-01-25 22:36:31,986 ===> loading pre-trained ...
2025-01-25 22:36:31,987 ===> Loading checkpoint '/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1-best_model.pth'
2025-01-25 22:36:32,143 The pretrained_model is at checkpoint 108.
2025-01-25 22:36:49,531 Test Overall Acc 0.9234, Its test avg acc 0.8825.
2025-01-25 22:38:37,040 saving log, checkpoint and back up code in folder: log
2025-01-25 22:38:37,041 ==========       args      =============
2025-01-25 22:38:37,041 phase:test
2025-01-25 22:38:37,041 exp_name:
2025-01-25 22:38:37,041 job_name:modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0
2025-01-25 22:38:37,041 use_cpu:False
2025-01-25 22:38:37,041 root_dir:log
2025-01-25 22:38:37,041 data_dir:Dataset
2025-01-25 22:38:37,041 dataset:ModelNet40
2025-01-25 22:38:37,041 num_points:2048
2025-01-25 22:38:37,041  :True
2025-01-25 22:38:37,041 in_channels:3
2025-01-25 22:38:37,041 batch_size:32
2025-01-25 22:38:37,041 epochs:400
2025-01-25 22:38:37,041 use_sgd:False
2025-01-25 22:38:37,041 weight_decay:0.0001
2025-01-25 22:38:37,041 lr:0.001
2025-01-25 22:38:37,041 seed:1
2025-01-25 22:38:37,041 multi_gpus:True
2025-01-25 22:38:37,041 test_batch_size:50
2025-01-25 22:38:37,041 pretrained_model:/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1-best_model.pth
2025-01-25 22:38:37,041 k:15
2025-01-25 22:38:37,041 block:dense
2025-01-25 22:38:37,041 conv:trans
2025-01-25 22:38:37,041 act:relu
2025-01-25 22:38:37,041 norm:batch
2025-01-25 22:38:37,041 bias:True
2025-01-25 22:38:37,041 n_blocks:6
2025-01-25 22:38:37,041 n_filters:256
2025-01-25 22:38:37,041 emb_dims:1024
2025-01-25 22:38:37,041 dropout:0.5
2025-01-25 22:38:37,041 dynamic:True
2025-01-25 22:38:37,041 fine_tune:False
2025-01-25 22:38:37,041 fine_tune_num_classes:16
2025-01-25 22:38:37,042 use_dilation:True
2025-01-25 22:38:37,042 epsilon:0.2
2025-01-25 22:38:37,042 use_stochastic:True
2025-01-25 22:38:37,042 device:cuda
2025-01-25 22:38:37,042 exp_dir:log
2025-01-25 22:38:37,042 res_dir:/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/result/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0
2025-01-25 22:38:37,042 loglevel:info
2025-01-25 22:38:37,042 ==========     args END    =============
2025-01-25 22:38:37,042 

2025-01-25 22:38:37,042 ===> Phase is test.
2025-01-25 22:38:37,047 ===> Creating data-loader ...
2025-01-25 22:38:40,670 ===> Loading ModelNet40 from Dataset. number of classes equal to 40
2025-01-25 22:38:40,670 ===> Loading the network ...
2025-01-25 22:38:40,974 ===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
├─DynamicEdgeConvLayer: 1-1                        --
|    └─Sequential: 2-1                             --
|    |    └─Linear: 3-1                            1,792
|    |    └─ReLU: 3-2                              --
|    |    └─Linear: 3-3                            65,792
|    |    └─ReLU: 3-4                              --
|    └─DynamicEdgeConv: 2-2                        --
|    |    └─MaxAggregation: 3-5                    --
|    |    └─Sequential: 3-6                        (recursive)
|    └─BatchNorm1d: 2-3                            512
├─Sequential: 1-2                                  --
|    └─DenseDynBlock2d: 2-4                        --
|    |    └─DynConv2d: 3-7                         526,080
|    └─DenseDynBlock2d: 2-5                        --
|    |    └─DynConv2d: 3-8                         788,224
|    └─DenseDynBlock2d: 2-6                        --
|    |    └─DynConv2d: 3-9                         1,050,368
|    └─DenseDynBlock2d: 2-7                        --
|    |    └─DynConv2d: 3-10                        1,312,512
|    └─DenseDynBlock2d: 2-8                        --
|    |    └─DynConv2d: 3-11                        1,574,656
├─BasicConv: 1-3                                   --
|    └─Conv2d: 2-9                                 5,505,024
|    └─LeakyReLU: 2-10                             --
|    └─BatchNorm2d: 2-11                           2,048
├─Sequential: 1-4                                  --
|    └─BasicConv: 2-12                             --
|    |    └─Conv2d: 3-12                           1,049,088
|    |    └─LeakyReLU: 3-13                        --
|    |    └─BatchNorm2d: 3-14                      1,024
|    |    └─Dropout2d: 3-15                        --
|    └─BasicConv: 2-13                             --
|    |    └─Conv2d: 3-16                           131,328
|    |    └─LeakyReLU: 3-17                        --
|    |    └─BatchNorm2d: 3-18                      512
|    |    └─Dropout2d: 3-19                        --
|    └─BasicConv: 2-14                             --
|    |    └─Conv2d: 3-20                           10,280
===========================================================================
Total params: 12,019,240
Trainable params: 12,019,240
Non-trainable params: 0
===========================================================================
2025-01-25 22:38:41,150 DataParallel(
  (module): DeepGCN(
    (dynamic_head): DynamicEdgeConvLayer(
      (mlp): Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      )
      (conv): DynamicEdgeConv(nn=Sequential(
        (0): Linear(in_features=6, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
      ), k=15)
      (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (backbone): Sequential(
      (0): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(256, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (1): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(512, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (2): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(768, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (3): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1024, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
      (4): DenseDynBlock2d(
        (body): DynConv2d(
          (gconv): GraphTransformerLayer(
            (transformer): TransformerConv(1280, 64, heads=4)
            (feed_forward): Sequential(
              (0): Linear(in_features=256, out_features=512, bias=True)
              (1): ReLU()
              (2): Dropout(p=0.2, inplace=False)
              (3): Linear(in_features=512, out_features=256, bias=True)
            )
            (activation): ReLU()
          )
          (dilated_knn_graph): DenseDilatedKnnGraph(
            (_dilated): DenseDilated()
          )
        )
      )
    )
    (fusion_block): BasicConv(
      (0): Conv2d(5376, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): LeakyReLU(negative_slope=0.2)
      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (prediction): Sequential(
      (0): BasicConv(
        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (1): BasicConv(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.2)
        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout2d(p=0.5, inplace=False)
      )
      (2): BasicConv(
        (0): Conv2d(256, 40, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
2025-01-25 22:38:41,154 ===> loading pre-trained ...
2025-01-25 22:38:41,154 ===> Loading checkpoint '/user/mahmoud.abdellahi/u12741/Project/deep_gcns_torch-master/examples/modelnet_cls/log/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1_20250104-040106_ab2f3a6a-27f3-42d2-8ff6-a048a7c458cf/checkpoint/modelnet_cls-dense-trans-n6-C256-norm_batch-k15-drop0.5-lr0.001-B32-seed1-best_model.pth'
2025-01-25 22:38:41,316 The pretrained_model is at checkpoint 108.
2025-01-25 22:38:58,660 Test Overall Acc 0.9234, Its test avg acc 0.8825.
